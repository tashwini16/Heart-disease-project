# -*- coding: utf-8 -*-
"""Heart_Diesease_predi _final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v-oa6zAEnCpzxMsiBbqrpPMc6BF4w0YK

# **Project team's ID :  " PPTID-CDS-FEB-24-1839"**

# **Project : PRCP-1016-HeartDieseasePred.**

# **INTRODUCTOIN**

Predicting and diagnosing heart disease is the biggest challenge in the medical industry and relies on factors such as the physical examination, symptoms and signs of the patient.

Factors that influence heart disease are body cholesterol levels, smoking habit and obesity, family history of illnesses, blood pressure, and work environment. Machine learning algorithms play an essential and precise role in the prediction of heart disease.

"Advances in technology allow machine language to combine with Big Data tools to manage unstructured and exponentially growing data. Heart disease is seen as the world’s deadliest disease of human life. In particular, in this type of disease, the heart is not able to push the required amount of blood to the remaining organs of the human body to perform regular functions."

Heart disease can be predicted based on various symptoms such as age, gender, heart rate, etc. and reduces the death rate of heart patients.

Due to the increasing use of technology and data collection, we can now predict heart disease using machine learning algorithms.

# Problem Statement

#Task 1 :- Prepare a complete data analysis report on the given data.

#Task 2 :- Create a model predicting potential Heart Diseases in people using Machine Learning algorithms.

#Task3 :- Suggestions to the Hospital  to awake the predictions of heart diseases  prevent life threats.

# Dataset

1.	There are 14 columns in the dataset, where the patient_id column is a
unique and random identifier. The remaining 13 features are described in the section below.
2.	•              slope_of_peak_exercise_st_segment (type: int): the slope of the peak exercise ST segment, an electrocardiography read out indicating quality of blood flow to the heart
3.	•              thal (type: categorical): results of thallium stress test measuring blood flow to the heart, with possible values normal, fixed_defect, reversible_defect
4.	•              resting_blood_pressure (type: int): resting blood pressure
5.	•              chest_pain_type (type: int): chest pain type (4 values)
6.	•              num_major_vessels (type: int): number of major vessels (0-3) colored by flourosopy
7.	•              fasting_blood_sugar_gt_120_mg_per_dl (type: binary): fasting blood sugar > 120 mg/dl
8.	•              resting_ekg_results (type: int): resting electrocardiographic results (values 0,1,2)
9.	•              serum_cholesterol_mg_per_dl (type: int): serum cholestoral in mg/dl
10.	•              oldpeak_eq_st_depression (type: float): oldpeak = ST depression induced by exercise relative to rest, a measure of abnormality in electrocardiograms
11.	•              sex (type: binary): 0: female, 1: male
12.	•              age (type: int): age in years
13.	•              max_heart_rate_achieved (type: int): maximum heart rate achieved (beats per minute)
14.	•              exercise_induced_angina (type: binary): exercise-induced chest pain (0: False, 1: True)
Mod

# Importing some necessary libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

data1=pd.read_csv('/content/values.csv')
data2=pd.read_csv('/content/labels.csv')

print(data1.columns)

print(data2.columns)

"""# **Merging 2 Data Sets Using 'patient_id'**"""

data=pd.merge(data1,data2,on='patient_id' ,how='inner')
data

"""# Basic Checks"""

data.columns

data.drop('patient_id',axis=1,inplace=True)

data.head()

data.shape

"""Here data set have 180 rows and 14 columns

"""

data.tail()

data.info()

"""1.  slope_of_peak_exercise_st_segment (int64): The slope of the peak exercise ST segment in the electrocardiogram.
2. thal (object): A categorical variable representing a type of thalassemia.
3.  resting_blood_pressure (int64): The resting blood pressure of the individual.

4. chest_pain_type (int64): The type of chest pain experienced by the individual.
5. num_major_vessels (int64): The number of major vessels colored by fluoroscopy.
6. fasting_blood_sugar_gt_120_mg_per_dl (int64): Presence of fasting blood sugar greater than 120 mg/dl.
7. resting_ekg_results (int64): The result of the resting electrocardiographic measurement.
8. serum_cholesterol_mg_per_dl (int64): The serum cholesterol level in mg/dl.
9. oldpeak_eq_st_depression (float64): ST depression induced by exercise relative to rest.
10. sex (int64): Gender of the individual (assuming 0 or 1 for male and female).
11. age (int64): Age of the individual.
12. max_heart_rate_achieved (int64): Maximum heart rate achieved during exercise.
13. exercise_induced_angina (int64): Presence of exercise-induced angina.
14. heart_disease_present (int64): The target variable indicating the presence or absence of heart disease.

The data seems to be related to heart health, with features that could be used to predict the presence of heart disease. The next steps would typically involve exploratory data analysis, data preprocessing, and then applying machine learning models to predict or analyze heart disease based on these features.
"""

data.duplicated().sum()

"""'0' duplicate rows present in a DataFrame or their is no duplicates found in dataset"""

data.isnull().sum()

"""So,we have no missing values"""

data.describe().T

data.columns

data_desc=data[['slope_of_peak_exercise_st_segment','thal','chest_pain_type',
                'num_major_vessels','fasting_blood_sugar_gt_120_mg_per_dl',
                'resting_ekg_results','sex']]

"""# **Exploratory Data Analysis(EDA)**

EDA stands for Exploratory Data Analysis. It's a crucial first step in data science projects that involves investigating and understanding a dataset before jumping into formal analysis or modeling.

EDA uses statistical and visualization techniques to reveal patterns and trends within the data.
"""

y = data["heart_disease_present"]

plt.figure(figsize=(15,15))
p=1

for i in data_desc:
  if p<=12:
    ax=plt.subplot(3,4,p)
    sns.countplot(x=data[i],hue=data.heart_disease_present)
  p+=1

"""
 The count plots for each categorical variable (data_desc) give insights into the distribution of different categories within each variable. This can help understand the prevalence of certain characteristics within the dataset.

By using the hue parameter to differentiate count plots based on the heart_disease_present variable, the visualizations allow for understanding how each categorical variable relates to the presence or absence of heart disease."""

sns.countplot(x='heart_disease_present', data=data)
target_temp = data.heart_disease_present.value_counts()
print(target_temp)
plt.show()

"""# **Percentage of patient with or without heart problems in the given dataset**"""

print("Percentage of patience without heart problems: "+str(round(target_temp[0]*100/180,2)))
print("Percentage of patience with heart problems: "+str(round(target_temp[1]*100/180,2)))

mypal= ['#FC05FB', '#FEAEFE', '#FCD2FC','#F3FEFA', '#B4FFE4','#3FFEBA']

plt.figure(figsize=(7, 5),facecolor='#F6F5F4')
total = float(len(data))
ax = sns.countplot(x=data['heart_disease_present'], palette=mypal[1::4])
ax.set_facecolor('#F6F5F4')

for p in ax.patches:

    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,height + 3,'{:1.1f} %'.format((height/total)*100), ha="center",
           bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.5))

ax.set_title('Heart variable distribution', fontsize=20, y=1.05)
sns.despine(right=True)
sns.despine(offset=5, trim=True)

"""Heart Disease frequency for sex (where 0 is female and 1 is male ) and "blue" is have heart disease and "green" is don't have heart disease"""

data["sex"].unique()

sns.countplot(x='sex', data=data, hue='heart_disease_present', palette='viridis')
plt.title('Distribution of Heart Disease by Gender')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(["Have Disease","Don't have Disease"])

plt.xlabel('Gender (0 = Female, 1 = Male)')
plt.show()

"""Insights

It show that heart disease is more prevalent in males than in females.

Male: 70 individuals have heart disease and 52 don't have it.
Female: 45 individuals have heart disease and 10 don't have it.

# **Plot accodring to AGE group**
"""

def plotAge(data):
    # Create subplots
    fig, axes = plt.subplots(2, 1, figsize=(15, 8))

    # KDE plot for 'age' variable
    sns.kdeplot(data[data['heart_disease_present'] == 0]['age'], label='Disease False', shade=True, ax=axes[0])
    sns.kdeplot(data[data['heart_disease_present'] == 1]['age'], label='Disease True', shade=True, ax=axes[0])
    axes[0].set(xlabel='Age', ylabel='Density')
    axes[0].set_title('KDE Plot for Age')
    axes[0].legend()

    # Bar plot for disease probability
    avg = data.groupby('age')['heart_disease_present'].mean().reset_index()
    sns.barplot(x='age', y='heart_disease_present', data=avg, ax=axes[1])
    axes[1].set(xlabel='Age', ylabel='Disease Probability')
    axes[1].set_title('Bar Plot for Disease Probability')

    # Adjust layout
    plt.tight_layout()

    # Show the plot
    plt.show()



# Call the function with your dataset
plotAge(data)

"""According to the data, the probability of heart disease is higher at ages 35 and 38 as well as ages 60 , 62 and 77.

# **Percentage of female and male patients**
"""

countFemale = len(data[data.sex == 0])
countMale = len(data[data.sex == 1])
print("Percentage of Female Patients:{:.2f}%".format((countFemale)/(len(data.sex))*100))
print("Percentage of Male Patients:{:.2f}%".format((countMale)/(len(data.sex))*100))

"""# **Separating categorical and continuous variables**"""

categorical_val = []
continous_val = []
for column in data.columns:
    print('==============================')
    print(f"{column} : {data[column].unique()}")
    if len(data[column].unique()) <= 10:
        categorical_val.append(column)
    else:
        continous_val.append(column)

"""# Overall, the code snippet is creating a visualization that compares the distribution of each categorical feature in the heart disease dataset between patients with and without heart disease."""

plt.figure(figsize=(15, 15))

for i, column in enumerate(categorical_val, 1):
    plt.subplot(3, 3, i)
    data[data["heart_disease_present"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)
    data[data["heart_disease_present"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)
    plt.legend()
    plt.xlabel(column)

"""# Distribution of each countinous feature in the heart disease dataset between patients with and without heart disease"""

plt.figure(figsize=(15, 15))

for i, column in enumerate(continous_val, 1):
    plt.subplot(3, 2, i)
    data[data["heart_disease_present"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)
    data[data["heart_disease_present"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)
    plt.legend()
    plt.xlabel(column)

"""# **A scatter plot to visualize the relationship between age and max heart rate for people with and without heart disease.**"""

# Create another figure
plt.figure(figsize=(10, 8))

# Scatter with postivie examples
plt.scatter(data.age[data.heart_disease_present==1],
            data.thal[data.heart_disease_present==1],
            c="salmon")

# Scatter with negative examples
plt.scatter(data.age[data.heart_disease_present==0],
            data.thal[data.heart_disease_present==0],
            c="lightblue")

# Add some helpful info
plt.title("Heart Disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"]);

"""# **Heart Disease Frequency for Slope_of_peak_exercise**"""

data["slope_of_peak_exercise_st_segment"].unique()

pd.crosstab(data.slope_of_peak_exercise_st_segment,data.heart_disease_present).plot(kind="bar",figsize=(15,5),color=['#DAF7A6','#FF5733' ])
plt.title('Heart Disease Frequency for Slope')
plt.xlabel('The Slope of The Peak Exercise ST Segment ')
plt.xticks(rotation = 0)
plt.ylabel('Frequency')
plt.show()

"""####  #Slope '1' causes heart pain much more than Slope '2' and '3'

# **Heart disease Frequency according to Fasting Blood sugar**
"""

data["fasting_blood_sugar_gt_120_mg_per_dl"].unique()

pd.crosstab(data.fasting_blood_sugar_gt_120_mg_per_dl,data.heart_disease_present).plot(kind="bar",figsize=(15,5),color=['#FFC300','#581845' ])
plt.title('Heart Disease Frequency According To FBS')
plt.xlabel('FBS - (Fasting Blood Sugar > 120 mg/dl) (1 = true; 0 = false)')
plt.xticks(rotation = 0)
plt.legend(["Haven't Disease", "Have Disease"])
plt.ylabel('Frequency of Disease or Not')
plt.show()

"""# **Heart Disease Frequency According To Chest Pain Type**

# **Analysing the chest pain (4 types of chest pain)**
## #[Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic]
"""

data["chest_pain_type"].unique()

pd.crosstab(data.chest_pain_type,data.heart_disease_present).plot(kind="bar",figsize=(15,6),color=['#11A5AA','#AA1190' ])
plt.title('Heart Disease Frequency According To Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.xticks(rotation = 0)
plt.ylabel('Frequency of Disease or Not')
plt.show()

"""# **Analysing The person's resting blood pressure (mm Hg on admission to the hospital)**


"""

data["resting_blood_pressure"].unique()

plt.figure(figsize=(26, 6))
sns.barplot(x=data["resting_blood_pressure"], y="heart_disease_present", data=data)
plt.title('Blood_pressure')
plt.xlabel('resting_blood_pressure')
plt.xticks(rotation = 0)
plt.ylabel('heart_disease_present')
plt.show()

"""# Analysing the Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)"""

data["resting_ekg_results"].unique()

plt.figure(figsize=(26, 10))
sns.barplot(x=data["resting_ekg_results"], y="heart_disease_present", data=data)
plt.title('Resting ECG Results')
plt.xlabel('Resting ECG Results')
plt.ylabel('Heart Disease Present')
plt.xticks(rotation=0)
plt.show()

"""# people with restecg '1' and '2' are much more likely to have a heart disease than with restecg '0'

# **Analysing Exercise induced angina (1 = yes; 0 = no)**
"""

data["exercise_induced_angina"].unique()

plt.figure(figsize=(10, 10))
sns.countplot(x="exercise_induced_angina", data=data)
plt.title('Count of Observations by Exercise Induced Angina')
plt.show()

"""##### #People with exercise_induced_angina=1 are much less likely to have heart problems.

# **Analysing number of major vessels (0-3) colored by flourosopy**
"""

data["num_major_vessels"].unique()

plt.figure(figsize=(10, 10))
sns.countplot(data=data, x='num_major_vessels')
plt.title('Distribution of Number of Major Vessels')
plt.xlabel('Number of Major Vessels')
plt.show()

"""# **Analysing A blood disorder called thalassemia ( normal; fixed defect;reversable defect)**"""

data["thal"].unique()

sns.histplot(data["thal"])

sns.pairplot(data=data)

fig, ax = plt.subplots(figsize=(15, 15))

# Plot histograms for each column of the dataframe
data.hist(ax=ax)

plt.show()

"""# **Correlation Matrix**

Correlation analysis is a method of statistical evaluation used to study the strength of a relationship between two, numerically measured, continuous variables.
"""

# Let's make our correlation matrix a little prettier
corr_matrix = data.corr()
fig, ax = plt.subplots(figsize=(10, 8))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidths=0.5,
                 fmt=".2f",
                 cmap="YlGnBu");
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5)

"""Their is no relationaship between the columns,hence no need to do drop the column"""

data.drop('heart_disease_present', axis=1).corrwith(data.heart_disease_present).plot(kind='bar', grid=True, figsize=(12, 8),
                                                   title="heart_disease_present ")

print(data.corr()["heart_disease_present"].abs().sort_values(ascending=False))

"""# **Creating Dummy Variables**
#Data Processing
After exploring the dataset, we can observe that we need to convert some variables to dummy variables and scale all values before training the machine learning models.
Since 'cp', 'thal' and 'slope' are categorical variables we'll turn them into dummy variables.

"""

a = pd.get_dummies(data['chest_pain_type'], prefix = "cp")
b = pd.get_dummies(data['thal'], prefix = "thal")
c = pd.get_dummies(data['slope_of_peak_exercise_st_segment'], prefix = "slope")

frames = [data, a, b, c]
data = pd.concat(frames, axis = 1)
data.head()

data = data.drop(columns = ['chest_pain_type', 'thal', 'slope_of_peak_exercise_st_segment'])
data.head()

from sklearn.preprocessing import StandardScaler
s_sc = StandardScaler()
col_to_scale = ['age', 'trestbps', 'serum_cholesterol_mg_per_dl', 'thal', 'oldpeak_eq_st_depression']

"""# **Applying Logistic Regression**
Now, I will train a machine learning model for the task of heart disease prediction. I will use the logistic regression algorithm as I mentioned at the beginning of the article.


But before training the model I will first define a helper function for printing the classification report of the performance of the machine learning model:
"""

# Split the dataset into features (y) and target (x)
X = data.drop('heart_disease_present', axis=1)
y = data.heart_disease_present

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Train Accuracy LogisticRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)

y_pred=lr_clf.predict(X_test)

accuracy_score(y_test,y_pred)
print("Train Accuracy : {:.2f}%".format(accuracy_score(y_test, y_pred)*100))

# Test Accuracy LogisticRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr_clf = LogisticRegression()
lr_clf.fit(X_test, y_test)

y_pred=lr_clf.predict(X_test)

accuracy_score(y_test,y_pred)
print("Test Accuracy : {:.2f}%".format(accuracy_score(y_test, y_pred)*100))

"""**SMOTE LOGISTIC REGRESSION**"""

from imblearn.over_sampling import SMOTE
sm=SMOTE()
x_smote,y_smote=sm.fit_resample(X_train,y_train)

y_smote.value_counts()

"""LOGISTIC REGRESSION AFTER SMOTE"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr= LogisticRegression()
lr.fit(x_smote, y_smote)

y_pred_smote=lr.predict(X_test)

accuracy_score(y_test,y_pred_smote)
print("Smote Accuracy : {:.2f}%".format(accuracy_score(y_test,y_pred_smote)*100))

"""Confusion Matrix LogisticRegression

The rows in a confusion matrix represent the actual labels of the data points, and the columns represent the labels that the algorithm predicted. The diagonal cells show the number of correct predictions, while the off-diagonal cells show the number of incorrect predictions.
"""

from sklearn.metrics import confusion_matrix
matrix= confusion_matrix(y_test, y_pred)
sns.heatmap(matrix,annot = True, fmt = "d")
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""Top Left (17): This cell shows that the model correctly predicted 17 data points that actually belong to class "0".

Top Right (2): This cell shows that the model incorrectly predicted class "1" for 2 data points that actually belong to class "0". This is also known as a false positive.

Bottom Left (7.5): This cell shows that the model incorrectly predicted class "0" for 7.5 data points that actually belong to class "1". This is also known as a false negative.

Bottom Right (22): This cell shows that the model correctly predicted 22 data points that actually belong to class "1"

# Precision Score and Classification report
"""

from sklearn.metrics import precision_score,classification_report
precision = precision_score(y_test, y_pred)
print("Precision: ",precision)
classification_report=classification_report(y_test, y_pred)
print(classification_report)

"""# **KNN Model**

KNN works based on the idea that similar data points tend to have similar characteristics. When making a prediction about a new data point, it considers the labels of the k closest data points in the training data set.
"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier

# Normalize the data
scaler = MinMaxScaler()
X_train_norm = scaler.fit_transform(X_train)
X_test_norm = scaler.transform(X_test)

# KNN Model
knn = KNeighborsClassifier(n_neighbors = 8)
knn.fit(X_train_norm, y_train)
y_pred_knn = knn.predict(X_test_norm)

print("KNN : {:.2f}%".format(knn.score(X_test_norm, y_test)*100))

"""Confusion matrix of KNN"""

matrix = confusion_matrix(y_test, y_pred_knn)
sns.heatmap(matrix, annot=True, fmt="d",cmap="Greens")
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import precision_score,classification_report
precision = precision_score(y_test, y_pred_knn)
print("Precision: ",precision)
classification_report=classification_report(y_test, y_pred_knn)
print(classification_report)

"""In this code, we first normalize the data using MinMaxScaler. Then, we create a KNN model with 8 neighbors and fit it to the normalized training data. We use the normalized test data to make predictions and calculate the accuracy score.

# **Support Vector Machine (SVM) Algorithm**

SVM is the Supervised Machine Learning algorithm used for both classification, regression. But mostly preferred for classification.

Given a dataset, the algorithm tries to divide the data using hyperplanes and then makes the predictions. SVM is a non-probabilistic linear classifier. While other classifiers, when classifying, predict the probability of a data point to belong to one group or the another, SVM directly says to which group the datapoint belongs to without using any probability calculation.
"""

from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm = SVC(kernel='rbf', C=10, gamma=0.01)
svm.fit(X_train_scaled, y_train)
prediction = svm.predict(X_test_scaled)

y_pred_scm = svm.predict(X_test_scaled)
print("SVM Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred_scm)*100))

"""Confusion matrix of SVM"""

matrix = confusion_matrix(y_test, y_pred_scm)
sns.heatmap(matrix, annot=True, fmt="d", cmap="Reds")
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""Precision score and classification report"""

from sklearn.metrics import precision_score,classification_report
precision = precision_score(y_test, y_pred_scm)
print("Precision: ",precision)
classification_report=classification_report(y_test, y_pred_scm)
print(classification_report)

"""We first normalize the data using StandardScaler. Then, we create an SVM model with a radial basis function kernel (RBF), regularization parameter C=10, and kernel coefficient gamma=0.01. The SVM model is then fit to the scaled training data and used to make predictions on the scaled test data. The accuracy score is calculated using the accuracy_score function from scikit-learn.

 "Note > that normalization is often recommended for SVM models, as they can be sensitive to the scale of the features. StandardScaler is a common choice for normalization, as it scales each feature to have zero mean and unit variance. However, other normalization methods like MinMaxScaler can also be used."

# **Naive Bayes**

Naive Bayes is a popular algorithm in machine learning used for classification tasks. It's based on Bayes' theorem and makes predictions based on probabilities.

Classifies new data points into predefined categories. Imagine sorting emails into spam or inbox, classifying documents by topic, or predicting if a patient has a certain disease.
"""

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)
acc = nb.score(X_test,y_test)*100

print("Accuracy of Naive Bayes: {:.2f}%".format(acc))

"""Confusion matrix of Naive Bayes





"""

matrix = confusion_matrix(y_test, y_pred_nb)
sns.heatmap(matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""Precision score and classification report"""

from sklearn.metrics import precision_score,classification_report
precision = precision_score(y_test, y_pred_nb)
print("Precision: ",precision)
classification_report=classification_report(y_test, y_pred_nb)
print(classification_report)

"""## **Random Forest**

Random forest is a powerful and versatile machine learning algorithm that excels in both classification and regression tasks. It belongs to the category of ensemble learning algorithms, which means it combines the predictions of multiple models to generate a more robust final prediction.
"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 1000, random_state = 1)
rf.fit(X_train ,y_train)
y_pred_rf = rf.predict(X_test)
acc = rf.score(X_test,y_test)*100
print("Random Forest Algorithm Accuracy Score : {:.2f}%".format(acc))

"""Confusion matrix of Random Forest"""

matrix = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(matrix, annot=True, fmt="d")
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""Precision score and classification report"""

from sklearn.metrics import precision_score,classification_report
precision = precision_score(y_test, y_pred_rf)
print("Precision: ",precision)
classification_report=classification_report(y_test, y_pred_rf)
print(classification_report)

"""# **Gradient Boosting**

Gradient boosting is a powerful machine learning technique that utilizes ensemble learning to achieve high accuracy in both classification and regression tasks. It's known for its effectiveness and is widely used in various applications.
"""

from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbc.fit(X_train, y_train)
y_pred_gb = gbc.predict(X_test)
acc = accuracy_score(y_test,y_pred_gb )*100
print("Gradient Boosting  Accuracy {:.2f}%".format(acc))

"""Confusion matrix of Gradient Boosting"""

matrix = confusion_matrix(y_test, y_pred_gb)
sns.heatmap(matrix, annot=True, fmt="d")
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""Precision score and classification report"""

from sklearn.metrics import precision_score,classification_report
precision = precision_score(y_test, y_pred_gb)
print("Precision: ",precision)
classification_report=classification_report(y_test, y_pred_gb)
print(classification_report)

"""# **Conclusion**

**Logistic Regression**: It performed reasonably well with and without SMOTE, achieving an accuracy of around 86-87%. SMOTE slightly reduced the accuracy, which might indicate that the class imbalance wasn't a significant issue for this model.

**K-Nearest Neighbors (KNN)**: KNN with Minmax scaling and 8 neighborhood points achieved an accuracy of 84.44%, indicating that it's a competitive model for this task.

**Support Vector Machine (SVM)**: The SVM model with an RBF kernel, C=10, and gamma=0.01 achieved an accuracy of 82.22%. It's slightly lower compared to other models but still performs reasonably well.

**Naive Bayes**: Despite its simplicity, Naive Bayes achieved an accuracy of 84.44%, which is comparable to more complex models like KNN and Logistic Regression.

**Random Forest**: Random Forest achieved an accuracy of 82.22%, which is slightly lower than some other models but still provides a decent performance.

**Gradient Boosting**: Similarly, Gradient Boosting achieved an accuracy of 82.22%, suggesting that while it may not outperform other models in this scenario, it still provides a reliable predictive performance.

In conclusion, from the models evaluated, Logistic Regression, KNN, Naive Bayes, SVM, Random Forest, and Gradient Boosting all demonstrated competitive performance in predicting the presence of heart disease. However, the choice of the best model would depend on various factors such as interpretability, computational resources, and specific requirements of the application.








"""

